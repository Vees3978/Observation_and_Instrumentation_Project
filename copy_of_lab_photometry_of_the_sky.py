# -*- coding: utf-8 -*-
"""Copy of lab-photometry-of-the-sky.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nuBHaNX5OPG8DOcRQ8WxQknnQJWP-BfU

<font color='gray'>

# Lab | Photometry of the Sky

This lab explores how to measure the brightness of objects and how to efficiently plan observatons. It is (an extremely friendly) competition to see how many stars your group can measure photometrically to a given precision within a given amount of time. The catalog `stars-at-10pc.csv` contains positions and archival magnitudes for 191 stars at a distance of about $\sf 10 pc$ from Earth. For this **photometry sprint**, your mission is to observe as many of these stars as possible within 1 hour, achieving at least S/N=100 on each star. You will need to plan which stars you want to observe and in what order, plan what filter and exposure times to use, observe carefully and quickly, and keep careful logs. This is not easy; there is no perfect approach. (This lab description was last updated 9 November 2023.)

*Please complete all sections of this lab together as a group. This `jupyter` notebook includes placeholders for responses in Code or Markdown cells, but feel free to add many more cells as needed. Collaborate with your team to construct a shared lab submission, including all your responses in one document.*

<font color='gray'>

## Learning Goals

After completing this lab, students should be able to:
- use biases, darks, and flatfields to calibrate an image
- measure the brightness of a star
- estimate the S/N of a photometric measurement
- plan an efficient telescope observing run
- organize and work with large datasets

<font color='gray'>

## Preparation:

*Do these steps before the start of your observing session.*

**1. Please read the instructions.** Make sure you know what to expect before the start of your observing session. With your lab group, talk through what to expect and what you need to do to prepare. This lab asks you to do substantial planning before arriving at the telescope; discussing your approach beforehand will make your observing run go more smoothly.

**2. Schedule your observing time.**  Please schedule 2-3 hours on any one of the three SBO telescopes Artemis (20"), Apollo (20"), or Leto (24"). Use the [appropriate Google Calendar](https://canvas.colorado.edu/courses/98189/pages/observing-tools?wrap=1) to book your time, including “ASTR3510 | Photometry of the Sky | {your group name}" as the title, and the names of your group members in the description. This lab requires on-sky observations, so you may need to reschedule your observing nights if interrupted by weather. Two groups can observe at the same time on the 20” deck, but nobody in ASTR3510 can observe while another class is scheduled on the 20" deck.

**3. Explore your target catalog.** The table `stars-at-10pc.csv` contains all stars from the [Gaia Catalog of Nearby Stars](https://ui.adsabs.harvard.edu/abs/2021A%26A...649A...6G/abstract) with distances between $\sf 9 pc$ and $\sf 11 pc$. The columns of this table include:
- `silly_name` = a name drawn randomly from US baby names
- `ra`, `dec` = the coordinates of the star in equinox "J2000.0" (= ICRS = they're measured relative to a spherical coordinate system precessed to the year 2000.0), with proper motions propagating positions relative to other stars into the "epoch" 2023.9
- `G`, `BP`, `RP`, `g`, `r`, `i`, `z`, `J`, `H`, `Ks`, `W1`, `W2`, `W3`, `W4` = apparent magnitudes of the star in various filters; the Gaia `G` magnitude broadly covers much of the optical and very near infrared and is probably a good general reference
- `distance` = distance to the star, in parsecs       
- `GaiaEDR3`= the index of the star in the Gaia Early Data Release 3 Catalog
- `names` = other names often used for this star, some of which may work to search for stars positions in TheSky

Start interacting with this catalog, either by printing it on paper, importing it into Excel/Sheets/Numbers, or loading it into python with `astropy.io.ascii.read`. The folder `stars-at-10pc-finders` contains finder charts with each of these stars at its center.

**4. Sketch your plan.** Discuss how you will optimize your 1-hour photometry sprint (details below). What preparation do you need? Which targets? What filter? What exposure times? How will you assess as you go? Briefly summarize some key ideas for how you'll be observing below:

<font color='gray'>

## Observations:

*Make sure to gather all the data you need during your observing session. Answer the discussion questions as you go; they are designed to help ensure you get useful data and understand what's going on. A checklist at the bottom of this section helps make sure you have all the data you need.*
"""



"""<font color='gray'>


**1. Prepare to collect data.**
- Follow the Telescope Opening Procedure to get ready to take imaging observations, including both setting the autosave to your team's directory in `SBO-Data-Share > Classes > ASTR3510` and focusing the telescope.   
- Start an observing log, on paper or digitally (templates [on Canvas](https://canvas.colorado.edu/courses/98189/pages/observing-tools?wrap=1)). The metadata that will be helpful to log for this lab include the weather, the image number, the time, the object, its Gaia `G` magnitude, the exposure time, the binning, and possibly comments to help plan future exposures. For this lab, you will be submitting your log, so please try to make it legible!

<font color='gray'>

**2. Test and prepare.**

- If you want, spend as much time as you want at the telescope *preparing* to start your photometry sprint. Gather any test exposures that might be useful and answer any questions you have before your start your timed observing. Make note of any useful information you learned below:
"""



"""<font color='gray'>

**3. Do your photometry sprint** = observe as many stars to S/N>100 observations as possible.
Here are the rules:
- Point the telescope at zenith.
- Start the clock, make note of the time, and start observing.
- Observe as many stars from the `stars-at-10pc.csv` table as you possibly can. Expose long enough on each target star to reach S/N=100 in aperture photometry, but short enough to avoid saturation. Use the same filter for all observations. All observations should be in 4x4 binning. If needed, you may combine multiple exposures together to reach the S/N.
- Keep a careful log, including at least the start times and exposure times of all science exposures.
- Stop observing stars after no more than 60 minutes.
- All stars observed to sufficient S/N during this 1-hour time window will count toward your total.
- Your grade is not set by the number of stars you successfuly observe; if you try your best, explain your work, and conduct careful analyses, you can earn full credit.
- **Have fun!** Prizes will be awarded not just for "most stars observed to S/N>100" but also for other categories such as (for example) "faintest", "reddest", "most crowded", "most southern", "weirdest image", "best log", "worst weather luck", or others.

<font color='gray'>

**4. Play.**
- Take one more moment to do something cool with the telescope on sky. Anything.
- Please describe what silly, delightful, or beautiful thing you did.

<font color='gray'>

**5. Gather calibrations.**
Gather calibration data, including the following:
- 15 flat fields through the filter you used for science observations (flat field lights on, lamp brightness and exposure times adjusted as needed)
- 5 bias exposures (lights off, telescope cover on, narrow-band filter like $\sf H\alpha$)
- 3 dark exposures (lights off, telescope cover on, narrow-band filter like $\sf H\alpha$, exposure time comparable to your longest science exposure)

<font color='gray'>

**6. Finish observing.**
- Make sure you have all the data you need in the Observations Checklist below.
- Follow the Telescope Closing Procedures to shut down the telescope.

<font color='gray'>

## Observations Checklist:
*Before leaving, make sure that you have all these images saved to the shared folder, so you can access them later. This is not necessarily a complete list of every exposure you might have taken, but it covers everything you need for Analysis.*

|  ?   | **Description**                                                        | **Details** |
|-----|------------------------------------------------------------------------|-------------|
|     | stars at 10pc, observed to sufficient S/N              | 4x4 binning |
|     | flat field calibrations  | 4x4 binning, same filter as science |
|     | bias calibrations                     | 4x4 binning, cover on |
|     | dark calibrations                     | 4x4 binning, cover on |

<font color='gray'>

## Analysis:

*Please complete the Analysis together as a group and collaborate to construct a shared lab submission presenting your results, embedded in this `jupyter` notebook. Write your responses so it will be readable as a standalone document that you can look back to a year or more from now and still find useful: use complete sentences and language that is easy to understand, show your work and include units on all calculations, clearly label the axes on all plots, include numerical colorbars on all images, and clearly comment code. You can have fun,
explore other ideas, or be silly in your reports, provided you at least answer the questions below.*
"""

!pip install photutils

from google.colab import drive
drive.mount('/content/drive', force_remount = True)

import matplotlib.pyplot as plt
from astropy.io import ascii
import pandas as pd
import matplotlib.pyplot as plt
import re
from astropy.io import fits
import numpy as np
from glob import glob
from photutils.aperture import CircularAperture, CircularAnnulus, ApertureStats
from astropy.stats import sigma_clipped_stats
from photutils.detection import DAOStarFinder

from astropy.wcs import WCS
from astropy.coordinates import SkyCoord
import astropy.units as u
import pandas as pd

"""<font color='gray'>

**1. Reflect.**
- Briefly describe your experience observing. What telescope did you use? What was your strategy? What worked well, and what didn't?

<font color='gray'>

**2. Define reference calibration images.**

- Calculate a median-stacked bias exposure $\sf E_{bias, median}$ [ADU] from your individual $\sf E_{bias}$ exposures. Display the image $\sf E_{bias, median}$  with an informative color scale and limits.

<font color='gray'>

- Calculate a median-stacked dark exposure $\sf E_{dark, median}$ [ADU] from your individual $\sf E_{dark}$ exposures. Use it to calculate the rate at which dark current accumulates in each pixel $\sf \dot{N}_{dark}$ [$\sf e^-/s$] as $$\sf \dot{N}_{dark} = g\cdot (E_{dark, median} - E_{bias, median})/t_{dark}$$ where $\sf g$ is the gain [$\sf e^-/ADU$] and $\sf t_{dark}$ is the dark exposure time [$\sf s$]. Display this $\sf \dot{N}_{dark}$ image with an informative color scale and limits.

<font color='gray'>

- Calculate a median-stacked flat-field exposure $\sf E_{flat, median}$ [ADU] from your individual $\sf E_{flat}$ exposures. Use it to calculate a normalized flat-field image $\sf I_{flat-field}$ [$\sf unitless$] as $$\sf I_{flat-field} =\frac{E_{flat, median} - E_{bias, median}}{median(E_{flat, median} - E_{bias, median})}$$ Display this $\sf I_{flat-field}$ image with an informative color scale and limits.
"""

def make_image(image, map='magma', percentiles=[1,99], title='title', xlab='x', ylab='y'):
    #plt.cla()
    plt.imshow(image, vmin=np.percentile(image, percentiles[0]), cmap=map, vmax=np.percentile(image, percentiles[1]))
    plt.colorbar()
    plt.title(title)
    plt.xlabel(xlab)
    plt.ylabel(ylab)
    plt.show() #fix others

def image_array(glob_command):
  files = glob(glob_command)
  image_list = []
  for f in files:
    this_image = fits.open(f)[0].data
    image_list.append(this_image)
  return np.array(image_list)

# import bias exps
bias_arr = image_array('/content/drive/MyDrive/Hot_Pixels!/Photometry_lab/Photometry_Lab_Noise/Artemis-Bias-*.fit')

# Calculate the median bias image
median_bias = np.median(bias_arr, axis=0)
make_image(median_bias,title='Median Bias')

# import dark exps
dark_arr_5= image_array('/content/drive/MyDrive/Hot_Pixels!/Photometry_lab/Photometry_Lab_Noise/Artemis-Dark-5*.fit')
dark_arr_10= image_array('/content/drive/MyDrive/Hot_Pixels!/Photometry_lab/Photometry_Lab_Noise/Artemis-Dark-10*.fit')

# Calculate the median dark image
median_dark_5 = np.median(dark_arr_5, axis=0)
median_dark_10 = np.median(dark_arr_10, axis=0)

# Calculate dark current rate
dark_10_header = fits.open('/content/drive/MyDrive/Hot_Pixels!/Photometry_lab/Photometry_Lab_Noise/Artemis-Dark-10.000secs-00000130.fit')[0].header
dark_5_header = fits.open('/content/drive/MyDrive/Hot_Pixels!/Photometry_lab/Photometry_Lab_Noise/Artemis-Dark-5.000secs-00000134.fit')[0].header

gain_dark_10 = dark_10_header['GAINADU']; t_dark_10 = dark_10_header['EXPTIME']
dark_rate_10 = (gain_dark_10 * (median_dark_10 - median_bias))/ t_dark_10
# do the same for 5?

# Display
make_image(dark_rate_10,title='Dark Current 10s')

# import flat field exps

flat_arr = image_array('/content/drive/MyDrive/Hot_Pixels!/Photometry_lab/Photometry_Lab_Noise/Artemis-FlatField-10.000secs-H-Alpha-*.fit')

# Calculate the normalized flat image
median_flat = np.median(flat_arr, axis=0) # median flat image
norm_flat = (median_flat - median_bias)/ np.median(median_flat - median_bias)

# Display
make_image(median_flat,title='Median Flat Field')

"""<font color='gray'>

**3. Calibrate your science data.**
- For all science images, calibrate your raw science exposure $\sf E_{light}$ into the number of photons recorded in each pixel $\sf N_{light}$ [$\sf e^-$] using the calibration equation $$\sf N_{light} = g\cdot(E_{light} - E_{bias, median}) - \dot{N}_{dark}\cdot t_{light} $$ where $\sf g$ is the gain [$\sf e^-/ADU$] and $\sf t_{light}$ is the exposure time of the light frame being calibrated [$\sf s$]. If you took all your science images in the same filter, you can use the same reference calibration images for all of them. Display these $\sf N_{light}$  images with informative color scales/limits.

<font color='gray'>

- For all science images, calculate the predicted uncertainty on the number of photons recorded in each pixel $\sf \sigma_{N_{light}}$ using the noise equation $$\sf \sigma_{N_{light}} = \sqrt{N_{light} + \dot{N}_{dark}\cdot t_{light} + \sigma_{RN,e^-}^2}$$ where the quadrature sum includes $\sf N_{light}$ as the Poisson noise from light in the image, $\sf \dot{N}_{dark}\cdot t_{light}$ as the Poisson noise from dark current, and $\sf \sigma_{RN,e^-}^2$ as the read-noise in electron units. Display these $\sf \sigma_{N_{light}}$ images with informative color scales/limits.
"""

def open_images_and_headers(glob_command):
  '''
  glob command: string of what you want imported
  '''
  files = glob(glob_command)
  image_list = []
  header_list = []
  names_list = []
  for f in files:
        with fits.open(f) as fitsfile:
            this_image = fitsfile[0].data
            this_header = fitsfile[0].header
            image_list.append(this_image)
            header_list.append(this_header)
            names_list.append(f.split('/')[-1].split('.')[0])
  return np.array(image_list), header_list, names_list

def make_image2(image, star_aperture, sky_aperture, map='magma', percentiles=[1,99], axis=plt):
    imy= axis.imshow(image, vmin=np.percentile(image, percentiles[0]), cmap=map, vmax=np.percentile(image, percentiles[1]))
    fig.colorbar(imy, ax=axis, fraction=0.046, pad=0.04)
    if star_aperture != None or sky_aperture != None:
      star_aperture.plot(color='white', ax=axis)
      sky_aperture.plot(color='white', linestyle='--', ax=axis);
    fig.tight_layout(pad=3.0)

def find_aperture(name):
    star_coords = [['Abigale', ['00h11m29.0s +59d08m12.0s']], ['Aliyah', ['00h12m13.6s +50d25m07.6s']], ['Evangelina', ['00h32m36.6s +67d13m57.8s']], ['Destin', ['00h32m36.7s +67d14m00.9s']], ['Ardeth', ['01h01m21.3s +61d21m37.3s']], ['Berta', ['01h02m41.4s +62d20m44.3s']], ['Maximillian', ['01h03m22.3s +62d21m57.9s']], ['Italy', ['01h09m51.8s -03d43m26.2s']], ['Scarlette', ['01h47m46.9s +63d51m03.1s']], ['Jasiah', ['01h51m05.0s -06d07m11.4s']], ['Kane', ['02h02m15.2s +10d20m07.2s']], ['Karrie', ['02h12m18.2s +03d33m48.0s']], ['Maricela', ['02h16m30.7s +13d35m02.5s']], ['Mikaila', ['02h17m05.4s +34d13m21.4s']], ['Katina', ['02h17m11.1s +35d26m26.3s']], ['Lorna', ['02h33m37.3s +24d55m21.6s']]]
    coords = None
    for star in star_coords:
        if star[0] == name:
            coords = star[1]
    if coords == None:
      return None,None
    sky_coord = SkyCoord(coords, unit=(u.hourangle, u.deg), obstime="J2000")
    wcs_header = fits.open('/content/drive/MyDrive/Hot_Pixels!/Photometry_lab/Photometry_Lab_WCS/'+name+'-wcs.fits')[0].header
    w = WCS(wcs_header)
    x, y = w.world_to_pixel(sky_coord)
    x=x[0]
    y=y[0]
    return CircularAperture([x,y], r=3), CircularAnnulus([x,y], r_in=5, r_out=10)

#destin 25 and evangeline 26 are the same image

# load light images
light_im, light_head, names = open_images_and_headers('/content/drive/MyDrive/Hot_Pixels!/Photometry_lab/Photometry_Lab_Light_Exposures/*-10.fit')

# Calculate Nlight for each
Nlights = []
Sig_Nlights = []
just_names = []
for exp in range(len(light_im)): # for each exposure
  t_light = light_head[exp]['EXPTIME']
  g_light = light_head[exp]['GAINADU']
  just_names.append(names[exp][:-5])

  light_shape = np.shape(light_im[exp])
  this_N_light = g_light*(light_im[exp] - median_bias[0:light_shape[0], 0:light_shape[1]]) - t_light * dark_rate_10[0:light_shape[0], 0:light_shape[1]] #why light_im and bias different shapes??
  Nlights.append(this_N_light)

  # Calculate the predicted uncertainty
  read_noise_adu = np.mean(np.std(bias_arr, axis=0)) # readout noise
  read_noise_electrons = read_noise_adu * g_light
  this_sigma_N_light_predicted = np.sqrt(read_noise_electrons**2  + (t_light*dark_rate_10[0:light_shape[0], 0:light_shape[1]]) + this_N_light)
  Sig_Nlights.append(this_sigma_N_light_predicted)

  star_aperture, sky_aperture = find_aperture(just_names[exp])

  # Display images
  fig, axs = plt.subplots(1,2, figsize=(12, 8))
  axs[0].set_title('Nlight, '+ names[exp])
  axs[1].set_title('Sigma Nlight, '+ names[exp])
  make_image2(this_N_light, axis=axs[0],star_aperture=star_aperture, sky_aperture=sky_aperture)
  make_image2(this_sigma_N_light_predicted, axis=axs[1], star_aperture=star_aperture, sky_aperture=sky_aperture)

"""<font color='gray'>

**3. Do photometry.**
- For each of your target 10pc stars, use the calibrated science images and aperture photometry to measure the total number of photons received from the star $\sf F$ = "fluence" = $\sf F = \sum N_{astronomical} = \sum (N_{light} - N_{sky})$ and the uncertainty on the fluence $\sf \sigma_{F}$ (which should be calculated automatically if you supply $\sf \sigma_{N_{light}}$ to the `photutils` photometry tools `error=` keyword argument). Assemble a table of measurements including the following columns

| Star Name |  $\sf F$  | $\sf \sigma_{F}$ | S/N = $\sf F/\sigma_{F}$ | $\sf t$ = exposure time | $\sf X$ = airmass | weather |
|-|-|-|-|-|-|-|
| (the name of the star) | (the fluence = the total number of photons from the star) | (the uncertainty on the fluence = how much we would expect the number of photons measured to change in repeat observations) | (the signal-to-noise ratio = how strongly we detect the object) | (the time is required to compare brightnesses across images with different exposure times) | (the airmass will help approximately correct extinction through Earth's clear atmosphere) | (were there clouds overhead that we should be aware of when interpreting this measurement?) |
|...|...|...|...|...|...|...|


"""

#for exp in range(len(light_im)):
  #print(names[exp])
  #np.shape(light_im[0])
  #sources = find_stars(light_im[exp][int(light_shape[0]/4):int(light_shape[0]*.75), int(light_shape[1]/4):int(light_shape[1]*.75)])
  #star_aps, sky_aps = create_apetures(sources)

# fluence

# do photometry on an image using an aperture
aperture_flux, aperture_uncertainty = star_aperture.do_photometry(f_image, error=u_image)

# calculate statistics on the pixels in the sky aperture
sky_stats = ApertureStats(f_image, sky_aperture)

# calculate an estimate of the sky per pixel
sky_per_pixel = sky_stats.median

# calculate how much sky flux must be in our star aperture
sky_flux = star_aperture.area*sky_per_pixel

# subtract the sky from the aperture flux
star_flux = aperture_flux[0] - sky_flux
star_uncertainty = aperture_uncertainty[0]

fractional_uncertainty = star_uncertainty/star_flux

signal_to_noise_ratio = star_flux/star_uncertainty
print(f'The S/N ratio is {signal_to_noise_ratio:.4}')

just_names = []; exp_times = []; airm_ass = []; weather = []

for exp in range(len(names)):
  just_names.append(names[exp][:-5])
  exp_times.append(light_head[exp]['EXPTIME'])
  airm_ass.append(light_head[exp]['AIRMASS'])
  weather.append('Clear')

# data of lists
data = {'Star Name': just_names,
        'Fluence': np.ones(len(just_names)),
        'Fluence Uncertainty': np.ones(len(just_names)),
        'S/N': np.ones(len(just_names)),
        'Exposure Time': exp_times,
        'Airmass': airm_ass,
        'Weather': weather}

# Create table !
Table = pd.DataFrame(data)
Table

"""<font color='gray'>

- Discuss how many stars you actually photometrically measured to S/N>100. What went right or what went wrong with your observing? (You are being graded on process and reflection here; you can still get full credit for this lab even if *none* of your stars meet S/N>100).

<font color='gray'>

**4. Convert to magnitudes.**

To translate photons detected at your telescope into astronomical magnitudes, we need to correct for (a) the sensitivity of the telescope/filter/camera and (b) the transparency of Earth's atmosphere. Starting from the fluence $\sf F$ [$\sf e^-$] of your object and the exposure time $\sf t$ [$\sf s$] of your observation, the magnitude can be calculated with either of the following two equations
$$\sf m_{\lambda} = -2.5\cdot log_{10}\left(\frac{F_{\lambda} /t}{\dot{F_{\lambda} }_{Vega}}\right) - k_\lambda \cdot X$$
$$\sf m_{\lambda} = m_{zeropoint, \lambda} -2.5\cdot log_{10}\left(\frac{F_{\lambda} /t}{1 e^-/s}\right) - k_\lambda \cdot X$$
where the sensitivity of your telescope is expressed with either $\sf \dot{F_{\lambda} }_{Vega}$ [$\sf e^-/s$] as the rate at which photons would get recorded from Vega or $\sf m_{zeropoint, \lambda}$ as the magnitude of an object from which we would record $\sf 1 e^-/s$, and the transparency of the atmosphere is expressed by the extinction coefficient $\sf k_\lambda$ [$\sf mag/airmass$] and the airmass $\sf X = 1/cos(\theta_{zenith})$ at which you observed. The $\sf \lambda$ subscripts throughout these equations indicate quantities that depend on filter being used; tables (coming soon!) will be provided with estimates of these quantities.

To translate uncertainties on your measurements into uncertainties on magnitudes $\sf \sigma_{m_\lambda}$, we can use propagation of errors to show that $$\sf \sigma_{m_\lambda} = (2.5 log_{10} e) \cdot \frac{\sigma_F}{F} = 1.086\frac{\sigma_F}{F}$$

- Calculate magnitudes $\sf m_\lambda$ and magnitude uncertainties $\sf \sigma_{m_\lambda}$ for all your 10pc star photometric measurements.

<font color='gray'>

- Discuss, qualitatively, whether your magnitudes make sense.

<font color='gray'>

**5. Make your own H-R diagram.**
- Make a Hertzsprung-Russell diagram using all the archival measurements from the original `stars-at-10pc.csv` table. For color on the $\sf x$-axis, use Gaia `BP` - `RP`. For absolute magnitude $\sf M$ on the $\sf y$-axis, use the Gaia `G` apparent magnitude. You can calculate a more precise absolute magnitude using the exact distance to the star, or simply assume everything is close to 10pc anyway. Make sure magnitude increases downward along the $\sf y$-axis, with brighter stars at the top.
- Plot your own measurements on top of these archival ones, using a contrasting color. Still use the archival `BP` - `RP` for color along the $\sf x$-axis, but replace `G` with your measured magnitudes for the $\sf y$-axis. Expect your magnitudes to differ from the Gaia `G` magnitudes because (among other reasons) you observed in a different filter.
"""

# get Gaia BP - RP
# get Gaia G apparent magnitude

"""<font color='gray'>

- Discuss what you notice. Do your measurements make sense? Are there types of stars you observed most? Are there types you missed?

<font color='gray'>

**6. Submit.**
Please plan to submit:
- a digital copy of your observing log
- a `.csv` table containing with one row for each star and columns for
    - star (silly) name
    - magnitude $\sf m_\lambda$
    - magnitude uncertainty $\sf \sigma{m_\lambda}$
    - filter used
- this lab notebook document

<font color='gray'>

*Run all the cells in your notebook from start to finish. Save your lab as an HTML file, which will embed any images and plots in a way they can be viewed in any browers. Have one member of your group submit it to Canvas. Each group member must individually submit the Lab Collaboration Statement on Canvas for this lab.*
"""